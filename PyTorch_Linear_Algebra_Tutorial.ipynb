{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Linear Algebra Tutorial\n",
        "\n",
        "A practical guide to linear algebra with **PyTorch**: tensors, GPU acceleration,\n",
        "`torch.linalg` ops, autograd with matrix calculus, batching, and sparse tensors.\n",
        "\n",
        "**Contents**\n",
        "1. Setup & tensors\n",
        "2. Device management (CPU/GPU) & dtype\n",
        "3. Basic ops, broadcasting, and aggregation\n",
        "4. Norms & distances\n",
        "5. Solving linear systems (`solve`, `pinv`, `lstsq`)\n",
        "6. Factorizations (QR / Cholesky / SVD)\n",
        "7. Eigenvalues/eigenvectors (`eig`, `eigh`)\n",
        "8. Least squares & regression (closed-form)\n",
        "9. Autograd with linear algebra\n",
        "10. Batched linear algebra\n",
        "11. Sparse tensors (preview)\n",
        "12. Tips: performance, precision, reproducibility\n",
        "13. Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup & tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic tensor creation\n",
        "x = torch.tensor([1., 2., 3.])\n",
        "X = torch.tensor([[1., 2.], [3., 4.]])\n",
        "zeros = torch.zeros(2, 3)\n",
        "ones = torch.ones(2, 3)\n",
        "eye = torch.eye(3)\n",
        "x, X, zeros.shape, ones.dtype, eye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Device management (CPU/GPU) & dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device, torch.get_default_dtype()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(6, dtype=torch.float32, device=device).reshape(2, 3)\n",
        "b = torch.arange(6, dtype=torch.float32, device=device).reshape(3, 2)\n",
        "c = a @ b\n",
        "c.device, c.dtype, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Basic ops, broadcasting, and aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u = torch.tensor([1., 2., 3.])\n",
        "v = torch.tensor([10., 20., 30.])\n",
        "u + v, u * v, v / u, u.pow(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Broadcasting example\n",
        "a = torch.tensor([1., 2., 3.]).reshape(3, 1)\n",
        "b = torch.tensor([10., 20., 30.]).reshape(1, 3)\n",
        "a + b  # result is 3x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "M = torch.arange(1., 10.).reshape(3, 3)\n",
        "M.sum(), M.mean(), M.std(), M.var(), M.min(), M.max(), M.mean(dim=0), M.mean(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Norms & distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "v = torch.tensor([3., -4., 12.])\n",
        "l1 = torch.linalg.vector_norm(v, ord=1)\n",
        "l2 = torch.linalg.vector_norm(v)\n",
        "linf = torch.linalg.vector_norm(v, ord=float('inf'))\n",
        "fro = torch.linalg.matrix_norm(M, ord='fro')\n",
        "l1, l2, linf, fro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Solving linear systems (`solve`, `pinv`, `lstsq`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = torch.tensor([[3., 2., -1.], [2., -2., 4.], [-1., 0.5, -1.]])\n",
        "b = torch.tensor([1., -2., 0.])\n",
        "x = torch.linalg.solve(A, b)\n",
        "torch.allclose(A @ x, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pseudoinverse-based solve (for rank-deficient/least squares)\n",
        "pinv = torch.linalg.pinv(A)\n",
        "x_pinv = pinv @ b\n",
        "x, x_pinv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Least squares (if available in your PyTorch version)\n",
        "try:\n",
        "    sol = torch.linalg.lstsq(A, b)\n",
        "    sol.solution, sol.rank\n",
        "except Exception as e:\n",
        "    str(e)  # fallback message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Factorizations (QR / Cholesky / SVD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QR factorization (economy)\n",
        "N = torch.tensor([[1., 1.], [1., -1.], [1., 2.]])\n",
        "Q, R = torch.linalg.qr(N, mode='reduced')\n",
        "beta = torch.linalg.solve(R, Q.T @ torch.tensor([2., 0., 3.]))\n",
        "beta, Q.shape, R.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cholesky for SPD matrices\n",
        "S = torch.tensor([[4., 2.], [2., 3.]])\n",
        "L = torch.linalg.cholesky(S)\n",
        "rhs = torch.tensor([1., 0.])\n",
        "y = torch.cholesky_solve(rhs.unsqueeze(1), L)\n",
        "y.squeeze(), torch.allclose(S @ y.squeeze(), rhs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVD\n",
        "A2 = torch.tensor([[1., 0., 0.], [0., 2., 0.], [0., 0., 0.5]]) @ torch.randn(3, 5)\n",
        "U, Svals, Vh = torch.linalg.svd(A2, full_matrices=False)\n",
        "rank = (Svals > 1e-12).sum()\n",
        "pinv_A2 = torch.linalg.pinv(A2)\n",
        "Svals, rank, pinv_A2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Eigenvalues/eigenvectors (`eig`, `eigh`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eigvals, eigvecs = torch.linalg.eig(A)   # may be complex\n",
        "eigvals_sym, eigvecs_sym = torch.linalg.eigh(S)  # symmetric\n",
        "eigvals, eigvals_sym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Least squares & regression (closed-form)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "X = torch.randn(200, 3)\n",
        "true_w = torch.tensor([2.0, -1.0, 0.5])\n",
        "y = X @ true_w + 0.1 * torch.randn(200)\n",
        "\n",
        "# Closed-form: w = (X^T X)^-1 X^T y  (using pinv for stability)\n",
        "w_hat = torch.linalg.pinv(X) @ y\n",
        "w_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Autograd with linear algebra\n",
        "Differentiate through linear algebra ops to fit models end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = torch.randn(3, requires_grad=True)\n",
        "optimizer = torch.optim.SGD([w], lr=0.1)\n",
        "for _ in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    loss = ((X @ w - y) ** 2).mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "w, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Batched linear algebra\n",
        "PyTorch can solve many small systems in parallel by adding a batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = 4\n",
        "A_batch = torch.randn(B, 3, 3)\n",
        "# Make them well-conditioned by A @ A^T\n",
        "A_batch = A_batch @ A_batch.transpose(-1, -2)\n",
        "b_batch = torch.randn(B, 3)\n",
        "x_batch = torch.linalg.solve(A_batch, b_batch)\n",
        "torch.allclose(A_batch @ x_batch.unsqueeze(-1), b_batch.unsqueeze(-1), atol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Sparse tensors (preview)\n",
        "PyTorch supports sparse layouts for memory efficiency with large sparse matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indices = torch.tensor([[0, 1, 1], [2, 0, 2]])  # COO indices\n",
        "values = torch.tensor([3., 4., 5.])\n",
        "sparse = torch.sparse_coo_tensor(indices, values, size=(2, 3))\n",
        "dense = torch.randn(3, 2)\n",
        "prod = torch.sparse.mm(sparse, dense)\n",
        "sparse, prod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Tips: performance, precision, reproducibility\n",
        "- Prefer vectorized/batched ops.\n",
        "- Move data and models to the **same device** (CPU vs GPU).\n",
        "- Consider **float64** for numerically sensitive LA tasks: `torch.set_default_dtype(torch.float64)`.\n",
        "- Use **mixed precision** (`torch.cuda.amp`) for speed on GPUs (careful with stability).\n",
        "- Set seeds for reproducibility: `torch.manual_seed(0)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Exercises\n",
        "1. **Solve Ax=b, three RHS**: Factor a random 100\u00d7100 SPD matrix (via `cholesky`) and solve for 3 random right-hand sides. Compare to `torch.linalg.solve`.\n",
        "2. **Least squares**: Create a collinear design matrix; compare `(X^T X)^{-1}X^T y` vs `torch.linalg.lstsq` or `pinv`.\n",
        "3. **Low-rank SVD**: Build a rank-2 matrix plus small noise; compute SVD and reconstruct the best rank-2 approximation; report Frobenius error.\n",
        "4. **Autograd + LA**: Fit a linear model with L2 regularization using autograd (ridge regression). Verify that increasing \u03bb shrinks weights.\n",
        "5. **Batched systems**: Generate a batch of SPD matrices of shape `(B, n, n)` and solve against a batch of RHS vectors in one call. Time vs loop.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}